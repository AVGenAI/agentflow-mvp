# Recommended Ollama Configuration for AgentFlow

# Primary Model (Best Quality)
LLM_PROVIDER=ollama
OLLAMA_MODEL=qwen2.5:14b
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Alternative Models (Comment/Uncomment as needed)

# For faster responses (less memory)
# OLLAMA_MODEL=qwen2.5:7b

# For highest quality (more memory)
# OLLAMA_MODEL=qwen2.5:32b

# For code-heavy tasks
# OLLAMA_MODEL=deepseek-coder-v2:16b

# For general purpose (good balance)
# OLLAMA_MODEL=llama3.1:8b

# Model-Specific Settings
OLLAMA_TEMPERATURE=0.7  # 0.7 for creative tasks, 0.3 for analytical
OLLAMA_MAX_TOKENS=4096  # Increase for longer responses
OLLAMA_CONTEXT_LENGTH=32768  # Qwen2.5 supports up to 128K